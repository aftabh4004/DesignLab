{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import locale\ndef getpreferredencoding(do_setlocale = True):\n    return \"UTF-8\"\nlocale.getpreferredencoding = getpreferredencoding","metadata":{"id":"xnSHsn5VmMi_","execution":{"iopub.status.busy":"2023-04-12T08:47:16.001500Z","iopub.execute_input":"2023-04-12T08:47:16.002555Z","iopub.status.idle":"2023-04-12T08:47:16.010232Z","shell.execute_reply.started":"2023-04-12T08:47:16.002510Z","shell.execute_reply":"2023-04-12T08:47:16.008924Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!pip install emoji==1.5.0","metadata":{"id":"PlqmEOWKmQ-u","outputId":"f833fae3-1919-40f8-d523-cc2dc666f106","execution":{"iopub.status.busy":"2023-04-12T08:47:16.012766Z","iopub.execute_input":"2023-04-12T08:47:16.013872Z","iopub.status.idle":"2023-04-12T08:47:26.544624Z","shell.execute_reply.started":"2023-04-12T08:47:16.013815Z","shell.execute_reply":"2023-04-12T08:47:26.543258Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Collecting emoji==1.5.0\n  Using cached emoji-1.5.0-py3-none-any.whl\nInstalling collected packages: emoji\n  Attempting uninstall: emoji\n    Found existing installation: emoji 1.7.0\n    Uninstalling emoji-1.7.0:\n      Successfully uninstalled emoji-1.7.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nemoji-translate 0.1.1 requires emoji==1.7.0, but you have emoji 1.5.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed emoji-1.5.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"pip install transformers","metadata":{"id":"fnemVZMi2mD8","outputId":"079d918c-63d4-429a-a068-debe427765b3","execution":{"iopub.status.busy":"2023-04-12T08:47:26.548758Z","iopub.execute_input":"2023-04-12T08:47:26.549208Z","iopub.status.idle":"2023-04-12T08:47:36.337170Z","shell.execute_reply.started":"2023-04-12T08:47:26.549146Z","shell.execute_reply":"2023-04-12T08:47:36.335939Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.27.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install emoji_translate","metadata":{"id":"g4GZoWrLmSPD","outputId":"ebb33a8a-9488-4548-898a-33b9a934c9ed","execution":{"iopub.status.busy":"2023-04-12T08:47:36.338873Z","iopub.execute_input":"2023-04-12T08:47:36.339270Z","iopub.status.idle":"2023-04-12T08:47:46.863400Z","shell.execute_reply.started":"2023-04-12T08:47:36.339215Z","shell.execute_reply":"2023-04-12T08:47:46.862229Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Requirement already satisfied: emoji_translate in /opt/conda/lib/python3.7/site-packages (0.1.1)\nCollecting emoji==1.7.0\n  Using cached emoji-1.7.0-py3-none-any.whl\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from emoji_translate) (1.3.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from emoji_translate) (1.21.6)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->emoji_translate) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->emoji_translate) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->emoji_translate) (1.16.0)\nInstalling collected packages: emoji\n  Attempting uninstall: emoji\n    Found existing installation: emoji 1.5.0\n    Uninstalling emoji-1.5.0:\n      Successfully uninstalled emoji-1.5.0\nSuccessfully installed emoji-1.7.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install pytorch-lightning -q -U","metadata":{"id":"S-m1NBtpL13h","outputId":"f3151fb7-270d-494e-9e62-d23da17f8abb","execution":{"iopub.status.busy":"2023-04-12T08:47:46.866986Z","iopub.execute_input":"2023-04-12T08:47:46.869310Z","iopub.status.idle":"2023-04-12T08:47:56.841956Z","shell.execute_reply.started":"2023-04-12T08:47:46.869257Z","shell.execute_reply":"2023-04-12T08:47:56.840792Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport emoji\nimport numpy as np\nfrom emoji_translate.emoji_translate import Translator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport torch.nn as nn\nimport torch\nimport torch.optim as optim\nimport pytorch_lightning as pl\nemo = Translator(exact_match_only=False, randomize=True)","metadata":{"id":"l1vsDJ30niw4","execution":{"iopub.status.busy":"2023-04-12T09:05:18.712995Z","iopub.execute_input":"2023-04-12T09:05:18.713437Z","iopub.status.idle":"2023-04-12T09:05:18.739970Z","shell.execute_reply.started":"2023-04-12T09:05:18.713400Z","shell.execute_reply":"2023-04-12T09:05:18.738982Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"# Intersection code, the DataFrame should be empty","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/testdatadeslab/hindi_test.csv\")\ndf_train = pd.read_csv(\"/kaggle/input/traindatadeslab/hindi_train_val.csv\")\n\ndf_test = df_test.merge(df_train, on=\"text\")\ndf_test# Intersection code, the DataFrame should be empty","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:05:18.761402Z","iopub.execute_input":"2023-04-12T09:05:18.761985Z","iopub.status.idle":"2023-04-12T09:05:18.889463Z","shell.execute_reply.started":"2023-04-12T09:05:18.761954Z","shell.execute_reply":"2023-04-12T09:05:18.888419Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [label_x, text, label_y]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label_x</th>\n      <th>text</th>\n      <th>label_y</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def pre_processing(line):\n    tokens = line.split()\n    processed_text = \" \".join(tokens)\n    processed_text = \"\".join((' '+c+' ') if c in emoji.UNICODE_EMOJI['en'] else c for c in processed_text)\n    processed_text = emo.demojify(processed_text)\n    return processed_text","metadata":{"id":"r0fKNCGVbQu1","execution":{"iopub.status.busy":"2023-04-12T09:05:18.891659Z","iopub.execute_input":"2023-04-12T09:05:18.892327Z","iopub.status.idle":"2023-04-12T09:05:18.898451Z","shell.execute_reply.started":"2023-04-12T09:05:18.892285Z","shell.execute_reply":"2023-04-12T09:05:18.897392Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"model_name = 'bert-base-multilingual-cased'\n","metadata":{"id":"hUDZBrU-_DY7","execution":{"iopub.status.busy":"2023-04-12T09:05:18.900105Z","iopub.execute_input":"2023-04-12T09:05:18.900853Z","iopub.status.idle":"2023-04-12T09:05:18.905378Z","shell.execute_reply.started":"2023-04-12T09:05:18.900816Z","shell.execute_reply":"2023-04-12T09:05:18.904256Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/lstmsample/train.csv\")\ndf['text'] = df['text'].apply(pre_processing)\nlabels = df[\"label\"].values.tolist()\ntext = df['text'].values.tolist()","metadata":{"id":"JvwpbLMgJiQQ","execution":{"iopub.status.busy":"2023-04-12T09:05:18.908146Z","iopub.execute_input":"2023-04-12T09:05:18.908888Z","iopub.status.idle":"2023-04-12T09:05:50.473765Z","shell.execute_reply.started":"2023-04-12T09:05:18.908850Z","shell.execute_reply":"2023-04-12T09:05:50.472649Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"train_text,val_text,train_label,val_label = train_test_split(text,labels,random_state=5,test_size=.2)\n# val_text,test_text,val_label,test_label = train_test_split(val_text,val_label,random_state=5,test_size=.2)","metadata":{"id":"bYwEObYinaTU","execution":{"iopub.status.busy":"2023-04-12T09:05:50.475061Z","iopub.execute_input":"2023-04-12T09:05:50.475448Z","iopub.status.idle":"2023-04-12T09:05:50.492137Z","shell.execute_reply.started":"2023-04-12T09:05:50.475409Z","shell.execute_reply":"2023-04-12T09:05:50.490961Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"# device (turn on GPU acceleration for faster execution)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(\"Device used: {}.\".format(device))","metadata":{"id":"m2Kw5svxpARX","outputId":"828d21ab-6d41-450f-abac-3ca486f2f35a","execution":{"iopub.status.busy":"2023-04-12T09:05:50.493895Z","iopub.execute_input":"2023-04-12T09:05:50.494345Z","iopub.status.idle":"2023-04-12T09:05:50.502312Z","shell.execute_reply.started":"2023-04-12T09:05:50.494307Z","shell.execute_reply":"2023-04-12T09:05:50.500860Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"Device used: cuda.\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import nn\nfrom transformers import AutoModel\n\nin_features = 768 # it's 768 because that's the size of the output provided by the underlying BERT model\n\nclass BertWithCustomNNClassifier(nn.Module):\n    \"\"\"\n    A pre-trained BERT model with a custom classifier.\n    The classifier is a neural network implemented in this class.\n    \"\"\"\n    \n    def __init__(self, linear_size):\n        super(BertWithCustomNNClassifier, self).__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.dropout1 = nn.Dropout(p=0.5)\n        self.linear1 = nn.Linear(in_features=in_features, out_features=linear_size)\n        self.batch_norm1 = nn.BatchNorm1d(num_features=linear_size)\n        self.dropout2 = nn.Dropout(p=0.8)\n        self.linear2 = nn.Linear(in_features=linear_size, out_features=1)\n        self.batch_norm2 = nn.BatchNorm1d(num_features=1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, tokens, attention_mask):\n        bert_output = self.bert(input_ids=tokens, attention_mask=attention_mask)\n        x = self.dropout1(bert_output[1])\n        x = self.linear1(x)\n        x = self.dropout2(x)\n        x = self.batch_norm1(x)\n        x = self.linear2(x)\n        x = self.batch_norm2(x)\n        return self.sigmoid(x)\n        \n    def freeze_bert(self):\n        \"\"\"\n        Freezes the parameters of BERT so when BertWithCustomNNClassifier is trained\n        only the wieghts of the custom classifier are modified.\n        \"\"\"\n        for param in self.bert.named_parameters():\n            param[1].requires_grad=False\n    \n    def unfreeze_bert(self):\n        \"\"\"\n        Unfreezes the parameters of BERT so when BertWithCustomNNClassifier is trained\n        both the wieghts of the custom classifier and of the underlying BERT are modified.\n        \"\"\"\n        for param in self.bert.named_parameters():\n            param[1].requires_grad=True\n\n            \nprint(BertWithCustomNNClassifier.__doc__)\n\n\nclass BertClassifier(nn.Module):\n  def __init__(self, dropout=0.5):\n    super(BertClassifier, self).__init__()\n    self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n    self.dropout = nn.Dropout(dropout)\n    self.linear = nn.Linear(768,2)\n    self.sigmod = nn.Sigmoid()\n  \n  def forward(self, input_id, mask):\n    _, pooled_output = self.bert(input_ids = input_id, attention_mask = mask, return_dict = False)\n    dropout_output = self.dropout(pooled_output)\n    linear_output = self.linear(dropout_output)\n    final_layer = self.sigmod(linear_output)\n    return final_layer","metadata":{"id":"6FLVfAK1nghl","outputId":"16413066-3ae1-4cd3-92dc-14009f963c26","execution":{"iopub.status.busy":"2023-04-12T09:05:50.504275Z","iopub.execute_input":"2023-04-12T09:05:50.505130Z","iopub.status.idle":"2023-04-12T09:05:50.523726Z","shell.execute_reply.started":"2023-04-12T09:05:50.505093Z","shell.execute_reply":"2023-04-12T09:05:50.522590Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"\n    A pre-trained BERT model with a custom classifier.\n    The classifier is a neural network implemented in this class.\n    \n","output_type":"stream"}]},{"cell_type":"code","source":"# parameters\nnum_of_epochs = 10\nlearning_rate = 1e-3\nbatch_size = 16\nhidden_layers = 8\n\nprint(\"Epochs: {}\".format(num_of_epochs))\nprint(\"Learning rate: {:.6f}\".format(learning_rate))\nprint(\"Batch size: {}\".format(batch_size))\nprint(\"The number of hidden layers in the custom head: {}\".format(hidden_layers))","metadata":{"id":"p-Vt3VAOsYPC","outputId":"1c2acc53-8604-40a8-fba6-2fc5b1527f55","execution":{"iopub.status.busy":"2023-04-12T09:05:50.528404Z","iopub.execute_input":"2023-04-12T09:05:50.528905Z","iopub.status.idle":"2023-04-12T09:05:50.536715Z","shell.execute_reply.started":"2023-04-12T09:05:50.528845Z","shell.execute_reply":"2023-04-12T09:05:50.535317Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"Epochs: 10\nLearning rate: 0.001000\nBatch size: 16\nThe number of hidden layers in the custom head: 8\n","output_type":"stream"}]},{"cell_type":"code","source":"model = BertWithCustomNNClassifier(linear_size=hidden_layers)\nmodel.to(device)","metadata":{"id":"FBDaN0NBsuel","outputId":"9c0fa4cb-bcc9-4fac-ec5c-11e4fe79fa42","execution":{"iopub.status.busy":"2023-04-12T09:05:50.538032Z","iopub.execute_input":"2023-04-12T09:05:50.538828Z","iopub.status.idle":"2023-04-12T09:06:17.110277Z","shell.execute_reply.started":"2023-04-12T09:05:50.538785Z","shell.execute_reply":"2023-04-12T09:06:17.109065Z"},"trusted":true},"execution_count":101,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2121ad3bc72744b887afb974fc10ad02"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"BertWithCustomNNClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout1): Dropout(p=0.5, inplace=False)\n  (linear1): Linear(in_features=768, out_features=8, bias=True)\n  (batch_norm1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout2): Dropout(p=0.8, inplace=False)\n  (linear2): Linear(in_features=8, out_features=1, bias=True)\n  (batch_norm2): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (sigmoid): Sigmoid()\n)"},"metadata":{}}]},{"cell_type":"code","source":"# from transformers import AdamW\n\n# # optimizer\n# optimizer = AdamW(model.parameters(), lr=learning_rate)\n# print('Initialized optimizer.')","metadata":{"id":"q32-snNms6or","outputId":"60b904e4-7586-4f90-e9e3-7485493db072","execution":{"iopub.status.busy":"2023-04-12T09:06:17.111920Z","iopub.execute_input":"2023-04-12T09:06:17.112581Z","iopub.status.idle":"2023-04-12T09:06:17.117265Z","shell.execute_reply.started":"2023-04-12T09:06:17.112539Z","shell.execute_reply":"2023-04-12T09:06:17.116112Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"# loss function\n# loss_fn = nn.BCELoss()\n# print('Initialized loss function.')","metadata":{"id":"zNxB8JbPtAkJ","outputId":"e7ffacb4-9522-4c0d-9455-8933a967de4a","execution":{"iopub.status.busy":"2023-04-12T09:06:17.118978Z","iopub.execute_input":"2023-04-12T09:06:17.119807Z","iopub.status.idle":"2023-04-12T09:06:17.124598Z","shell.execute_reply.started":"2023-04-12T09:06:17.119768Z","shell.execute_reply":"2023-04-12T09:06:17.123455Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"# from transformers import AutoTokenizer\n\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\n# train_encoding = tokenizer(train_text, truncation=True, padding=True,max_length=512,return_tensors=\"pt\")\n# val_encoding = tokenizer(val_text, truncation=True, padding=True,max_length=512,return_tensors=\"pt\")\n# test_encoding = tokenizer(test_text, truncation=True, padding=True,max_length=512,return_tensors=\"pt\")","metadata":{"id":"mmllbmrlYaHM","execution":{"iopub.status.busy":"2023-04-12T09:06:17.126250Z","iopub.execute_input":"2023-04-12T09:06:17.126937Z","iopub.status.idle":"2023-04-12T09:06:17.132914Z","shell.execute_reply.started":"2023-04-12T09:06:17.126899Z","shell.execute_reply":"2023-04-12T09:06:17.131635Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"# print(type(train_encodings[0][\"input_ids\"]))\n# train_encoding = train_encoding[0]\n#  val_encodings = val_encodings[0]","metadata":{"id":"1v5WZKC1vKWg","execution":{"iopub.status.busy":"2023-04-12T09:06:17.134665Z","iopub.execute_input":"2023-04-12T09:06:17.135498Z","iopub.status.idle":"2023-04-12T09:06:17.140218Z","shell.execute_reply.started":"2023-04-12T09:06:17.135462Z","shell.execute_reply":"2023-04-12T09:06:17.138948Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nclass SentimentDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Class to store the Sentece data as PyTorch Dataset\n    \"\"\"\n    \n    def __init__(self,df):\n        self.labels = df['label']\n        self.texts = [tokenizer(\n        text,\n        max_length = 256,\n        truncation = True,\n        return_tensors = \"pt\",\n        padding = 'max_length'\n        ) for text in df['text']]\n\n    def __getitem__(self,idx):\n        batch_texts = self.get_batch_texts(idx)\n        batch_labels = self.get_batch_labels(idx)\n        return batch_texts, batch_labels\n    def __len__(self):\n        return len(self.labels)\n    def classes(self):\n        return self.labels\n\n    def get_batch_texts(self, idx):\n        return self.texts[idx]\n\n    def get_batch_labels(self,idx):\n        return np.array(self.labels[idx])    \n\nprint(SentimentDataset.__doc__)","metadata":{"id":"EFCEz905cl94","outputId":"e6b5e577-e63b-40f4-802d-16b5f705ac34","execution":{"iopub.status.busy":"2023-04-12T09:08:39.915713Z","iopub.execute_input":"2023-04-12T09:08:39.916082Z","iopub.status.idle":"2023-04-12T09:08:42.003045Z","shell.execute_reply.started":"2023-04-12T09:08:39.916050Z","shell.execute_reply":"2023-04-12T09:08:42.001856Z"},"trusted":true},"execution_count":122,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d8471e61c4841c19de5e9fef0f2ce95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c8e1945fa034cd0af767881a193c80f"}},"metadata":{}},{"name":"stdout","text":"\n    Class to store the Sentece data as PyTorch Dataset\n    \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Hindi-BERT","metadata":{"id":"RJ6bPCP2mJnL"}},{"cell_type":"code","source":"# from torch.utils.data import DataLoader\n\n# # Dataset & dataloader\n# train_dataset = SentimentDataset(train_encoding, train_label)\n# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# val_dataset = SentimentDataset(val_encoding, val_label)\n# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n# print('Created train & val datasets.')\n# # test_dataset = SentimentDataset(test_encoding, val_label)\n# # test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)","metadata":{"id":"riiZm5XstL5H","outputId":"2ebaa51b-d3cc-40b4-848c-3a7ce3995d11","execution":{"iopub.status.busy":"2023-04-12T09:08:42.005483Z","iopub.execute_input":"2023-04-12T09:08:42.006236Z","iopub.status.idle":"2023-04-12T09:08:42.012644Z","shell.execute_reply.started":"2023-04-12T09:08:42.006178Z","shell.execute_reply":"2023-04-12T09:08:42.011276Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"def training_step(dataloader, model, optimizer, loss_fn, if_freeze_bert):\n    \"\"\"Method to train the model\"\"\"\n    \n    model.train()\n    model.freeze_bert() if if_freeze_bert else model.unfreeze_bert()\n      \n    epoch_loss = 0\n    size = len(dataloader.dataset)\n \n    for i, batch in enumerate(dataloader):        \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n    \n        outputs = torch.flatten(model(tokens=input_ids, attention_mask=attention_mask))\n                        \n        optimizer.zero_grad()\n        loss = loss_fn(outputs, labels.float())\n        epoch_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n            \nprint(training_step.__doc__)","metadata":{"id":"8D076fsytok4","outputId":"21e240cd-ca18-4dae-f290-75a783746149","execution":{"iopub.status.busy":"2023-04-12T09:08:42.014243Z","iopub.execute_input":"2023-04-12T09:08:42.014922Z","iopub.status.idle":"2023-04-12T09:08:42.026265Z","shell.execute_reply.started":"2023-04-12T09:08:42.014884Z","shell.execute_reply":"2023-04-12T09:08:42.024125Z"},"trusted":true},"execution_count":124,"outputs":[{"name":"stdout","text":"Method to train the model\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import f1_score, accuracy_score\n\ndef eval_prediction(y_batch_actual, y_batch_predicted):\n    \"\"\"Return batches of accuracy and f1 scores.\"\"\"\n    y_batch_actual_np = y_batch_actual.cpu().detach().numpy()\n    y_batch_predicted_np = np.round(y_batch_predicted.cpu().detach().numpy())\n    \n    acc = accuracy_score(y_true=y_batch_actual_np, y_pred=y_batch_predicted_np)\n    f1 = f1_score(y_true=y_batch_actual_np, y_pred=y_batch_predicted_np, average='weighted')\n    \n    return acc, f1\n\n# print(eval_prediction.__doc__)\ndef validation_step(dataloader, model, loss_fn):\n    \"\"\"Method to test the model's accuracy and loss on the validation set\"\"\"\n    \n    model.eval()\n    model.freeze_bert()\n    \n    size = len(dataloader)\n    f1, acc = 0, 0\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            X = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            y = batch['labels'].to(device)\n                  \n            pred = model(tokens=X, attention_mask=attention_mask)\n            \n            acc_batch, f1_batch = eval_prediction(y.float(), pred)                        \n            acc += acc_batch\n            f1 += f1_batch\n\n        acc = acc/size\n        f1 = f1/size\n                \n    return acc, f1\n        \nprint(validation_step.__doc__)","metadata":{"id":"Nzdl_rhRtw4k","outputId":"9ff87dac-7d73-4fe0-9cc8-a7e9bda03174","execution":{"iopub.status.busy":"2023-04-12T09:08:42.029388Z","iopub.execute_input":"2023-04-12T09:08:42.029872Z","iopub.status.idle":"2023-04-12T09:08:42.042647Z","shell.execute_reply.started":"2023-04-12T09:08:42.029836Z","shell.execute_reply":"2023-04-12T09:08:42.041512Z"},"trusted":true},"execution_count":125,"outputs":[{"name":"stdout","text":"Method to test the model's accuracy and loss on the validation set\n","output_type":"stream"}]},{"cell_type":"code","source":"# from tqdm.auto import tqdm\n\n# tqdm.pandas()\n\n# best_acc, best_f1 = 0, 0\n# path = './best_model.pt'\n# if_freeze_bert = False\n\n# for i in tqdm(range(num_of_epochs)):\n#     print(\"Epoch: #{}\".format(i+1))\n\n# #     if i < 5:\n# #         if_freeze_bert = False\n# #         print(\"Bert is not freezed\")\n# #     else:\n# #         if_freeze_bert = True\n# #         print(\"Bert is freezed\")\n    \n#     training_step(train_loader, model,optimizer, loss_fn, if_freeze_bert)\n#     train_acc, train_f1 = validation_step(train_loader, model, loss_fn)\n#     val_acc, val_f1 = validation_step(val_loader, model, loss_fn)\n    \n#     print(\"Training results: \")\n#     print(\"Acc: {:.3f}, f1: {:.3f}\".format(train_acc, train_f1))\n    \n#     print(\"Validation results: \")\n#     print(\"Acc: {:.3f}, f1: {:.3f}\".format(val_acc, val_f1))\n    \n#     if val_acc > best_acc:\n#         best_acc = val_acc    \n        \n#         checkpoint = {'model': model,\n#           'state_dict': model.state_dict()}\n\n#         torch.save(checkpoint, 'checkpoint.pth')","metadata":{"id":"Fsp7fZvfl7NA","outputId":"0377a818-f04f-48c6-fce4-a6e6851b48c7","execution":{"iopub.status.busy":"2023-04-12T09:08:42.044399Z","iopub.execute_input":"2023-04-12T09:08:42.044819Z","iopub.status.idle":"2023-04-12T09:08:42.054942Z","shell.execute_reply.started":"2023-04-12T09:08:42.044717Z","shell.execute_reply":"2023-04-12T09:08:42.053864Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\ndef evaluate(model, test_data):\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    \n    test = SentimentDataset(test_data)\n\n    test_dataloader = torch.utils.data.DataLoader(test, batch_size=64)\n\n\n\n    if use_cuda:\n\n        model = model.cuda()\n\n    total_acc_test = 0\n    y_pred = []\n    y_true = []\n    with torch.no_grad():\n\n        for test_input, test_label in test_dataloader:\n              optim_labels = test_label\n              optim_ids = test_input['input_ids']\n              mask = test_input['attention_mask'].to(device)\n              test_label = test_label.to(device)\n              input_id = test_input['input_ids'].squeeze(1).to(device)\n\n              output = model(input_id, mask)\n              y_pred.extend(output.argmax(dim=1).tolist())\n              y_true.extend(test_label.tolist())\n              acc = (output.argmax(dim=1) == test_label).sum().item()\n              total_acc_test += acc\n        \n    \n    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n    print(f'Test F1 Score: {f1_score(y_true, y_pred): .3f}')\n\n#df_test.reset_index(drop = True,inplace = True)\n#evaluate(model, df_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:08:42.057049Z","iopub.execute_input":"2023-04-12T09:08:42.057477Z","iopub.status.idle":"2023-04-12T09:08:42.068893Z","shell.execute_reply.started":"2023-04-12T09:08:42.057442Z","shell.execute_reply":"2023-04-12T09:08:42.067617Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    for parameter in model.parameters():\n        parameter.requires_grad = False\n\n    model.eval()\n    return model\n\nmodel = load_checkpoint('/kaggle/input/lstmsamplemodel/checkpoint.pth')","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:08:42.070333Z","iopub.execute_input":"2023-04-12T09:08:42.072407Z","iopub.status.idle":"2023-04-12T09:08:42.673714Z","shell.execute_reply.started":"2023-04-12T09:08:42.072369Z","shell.execute_reply":"2023-04-12T09:08:42.672536Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/testdatadeslab/hindi_test.csv\")\n# df = pd.read_csv('/kaggle/input/dlabdata/hindi_train_val.csv')\nX_eval = df.drop(['label'], axis = 1)\ny_eval = df['label']\ndf_eval = pd.concat([X_eval, y_eval], axis=1)\ndf_eval.reset_index(drop = True,inplace = True)\nevaluate(model, df_eval)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:08:42.676498Z","iopub.execute_input":"2023-04-12T09:08:42.677315Z","iopub.status.idle":"2023-04-12T09:09:35.447089Z","shell.execute_reply.started":"2023-04-12T09:08:42.677274Z","shell.execute_reply":"2023-04-12T09:09:35.445906Z"},"trusted":true},"execution_count":129,"outputs":[{"name":"stdout","text":"Test Accuracy:  0.817\nTest F1 Score:  0.802\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}}]}