{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install pytorch-lightning -q -U\n!pip install emoji==1.5.0\n\n\nimport torch\n\nimport pandas as pd\nimport nltk\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nnltk.download('punkt')\n\nimport re\nimport pickle\nimport emoji\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pytorch_lightning as pl\n\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom sklearn.metrics import classification_report\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-12T09:14:28.701209Z","iopub.execute_input":"2023-04-12T09:14:28.702576Z","iopub.status.idle":"2023-04-12T09:14:48.619484Z","shell.execute_reply.started":"2023-04-12T09:14:28.702522Z","shell.execute_reply":"2023-04-12T09:14:48.618106Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: emoji==1.5.0 in /opt/conda/lib/python3.7/site-packages (1.5.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Intersection code, the DataFrame should be empty","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/testdatadeslab/hindi_test.csv\")\ndf_train = pd.read_csv(\"/kaggle/input/traindatadeslab/hindi_train_val.csv\")\n\ndf_test = df_test.merge(df_train, on=\"text\")\ndf_test","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:14:48.622980Z","iopub.execute_input":"2023-04-12T09:14:48.623310Z","iopub.status.idle":"2023-04-12T09:14:48.755211Z","shell.execute_reply.started":"2023-04-12T09:14:48.623275Z","shell.execute_reply":"2023-04-12T09:14:48.753984Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [label_x, text, label_y]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label_x</th>\n      <th>text</th>\n      <th>label_y</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"TEST = True\nfile = \"/kaggle/input/testdatadeslab/hindi_test.csv\"\n\n# TEST = False\n# file = \"/kaggle/input/traindatadeslab/hindi_train_val.csv\"","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:14:48.756944Z","iopub.execute_input":"2023-04-12T09:14:48.757405Z","iopub.status.idle":"2023-04-12T09:14:48.762779Z","shell.execute_reply.started":"2023-04-12T09:14:48.757367Z","shell.execute_reply":"2023-04-12T09:14:48.761618Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"word_to_index = {\"<PAD>\": 0, \"<UNK>\": 1}\nSEQ_LEN = 20\n\n\nEMBEDDING_DIM = 512\nHIDDEN_DIM    = 256\nNUM_EPOCHS    = 100\nBATCH_SIZE    = 32","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:14:48.764482Z","iopub.execute_input":"2023-04-12T09:14:48.765410Z","iopub.status.idle":"2023-04-12T09:14:48.783608Z","shell.execute_reply.started":"2023-04-12T09:14:48.765366Z","shell.execute_reply":"2023-04-12T09:14:48.782579Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Helper Function","metadata":{}},{"cell_type":"code","source":"\ndef extract_emojis(s):\n    output = ''.join((' '+c+' ') if c in emoji.UNICODE_EMOJI['en'] else c for c in s)\n    output = emoji.demojize(output).replace(\":\", \"\")\n    return output\n\ndef word_mapping_train(sentence):\n    padding = np.zeros(SEQ_LEN)\n    mapping = []\n    for word in sentence:\n        try:\n            mapping.append(word_to_index[word])\n        except:\n            word_to_index[word] = len(word_to_index)\n            mapping.append(word_to_index[word])\n    mapping = np.array(mapping[:SEQ_LEN])\n    mapping = np.pad(mapping, (0, SEQ_LEN-len(mapping)), 'constant', constant_values=(0, 0))    \n\n    return mapping\n\ndef word_mapping_test(sentence):\n    padding = np.zeros(SEQ_LEN)\n    mapping = []\n    for word in sentence:\n        try:\n            mapping.append(word_to_index[word])\n        except:\n            mapping.append(1)\n    mapping = np.array(mapping[:SEQ_LEN])\n    mapping = np.pad(mapping, (0, SEQ_LEN-len(mapping)), 'constant', constant_values=(0,0))    \n\n    return mapping\n","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:14:48.787614Z","iopub.execute_input":"2023-04-12T09:14:48.787918Z","iopub.status.idle":"2023-04-12T09:14:48.797336Z","shell.execute_reply.started":"2023-04-12T09:14:48.787890Z","shell.execute_reply":"2023-04-12T09:14:48.796085Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"def load_data(path):\n    df = pd.read_csv(path)\n    return df\n\ndef process_data(df, test=False):\n    df[\"text\"] = df[\"text\"].apply(extract_emojis)\n    df[\"text\"] = df[\"text\"].apply(nltk.word_tokenize)\n    if not test:\n        df[\"text\"] = df[\"text\"].apply(word_mapping_train)\n    else:\n        df[\"text\"] = df[\"text\"].apply(word_mapping_test)\n\n\n    X = torch.tensor(list(df[\"text\"].values))\n    \n    Y = torch.tensor(list(df[\"label\"]))\n    return X, Y","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:14:48.798948Z","iopub.execute_input":"2023-04-12T09:14:48.799341Z","iopub.status.idle":"2023-04-12T09:14:48.809269Z","shell.execute_reply.started":"2023-04-12T09:14:48.799294Z","shell.execute_reply":"2023-04-12T09:14:48.808421Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"\n\nclass ATDModel(pl.LightningModule):\n    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim) \n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, num_layers=num_layers, bidirectional=False)\n        self.dropout1 = nn.Dropout()\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n        self.batch_norm1 = nn.BatchNorm1d(num_features=hidden_dim)\n\n        self.dropout2 = nn.Dropout(0.8)\n        self.fc2 = nn.Linear(hidden_dim, tagset_size)\n        self.batch_norm2 = nn.BatchNorm1d(num_features=tagset_size)\n\n        self.loss_fn = nn.BCELoss()\n    \n    def forward(self, x):\n        \n        embeds = self.embedding(x)\n\n        lstm_out, _ = self.lstm(embeds)\n        tag_space = self.fc1(lstm_out[:, -1, :])\n        tag_space = self.fc2(tag_space)\n        tag_scores = nn.functional.sigmoid(tag_space)\n        return tag_scores\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.forward(x)\n\n\n        y = y.type(torch.FloatTensor)\n        y_hat = y_hat.type(torch.FloatTensor)\n\n        loss = self.loss_fn(y_hat.view(-1), y.view(-1))\n        # loss = self.loss_fn(y_hat, y.unsqueeze(dim=1))\n\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.forward(x)\n\n\n        y = y.type(torch.FloatTensor)\n        y_hat = y_hat.type(torch.FloatTensor)\n\n        loss = self.loss_fn(y_hat.view(-1), y.view(-1))\n        # loss = self.loss_fn(y_hat, y.unsqueeze(dim=1))\n\n        self.log('val_loss', loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.forward(x)\n\n        y = y.type(torch.FloatTensor)\n\n        y_hat = y_hat.type(torch.FloatTensor)\n        loss = self.loss_fn(y_hat.view(-1), y.view(-1))\n        # loss = self.loss_fn(y_hat, y.unsqueeze(dim=1))\n\n        self.log('test_loss', loss)\n        return loss\n    \n  \n    def configure_optimizers(self):\n            optimizer = optim.Adam(self.parameters())\n            return optimizer","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:14:48.810415Z","iopub.execute_input":"2023-04-12T09:14:48.812870Z","iopub.status.idle":"2023-04-12T09:14:48.828439Z","shell.execute_reply.started":"2023-04-12T09:14:48.812840Z","shell.execute_reply":"2023-04-12T09:14:48.827488Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Dataloaders","metadata":{}},{"cell_type":"code","source":"def train_dataloader():\n    df = load_data(file)\n    X, y = process_data(df)\n    \n    X_train, X_val, y_train, y_val = train_test_split(X, y,  random_state=42, stratify = y)\n    \n    train_dataset = TensorDataset(X_train, y_train)\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    \n    val_dataset = TensorDataset(X_val, y_val)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n\n    return train_loader, val_loader\n\ndef test_dataloader():\n    df = load_data(file)\n    X, y = process_data(df, test=True)\n    \n    test_dataset = TensorDataset(X, y)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n    \n    \n    return test_loader","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:14:48.831957Z","iopub.execute_input":"2023-04-12T09:14:48.832275Z","iopub.status.idle":"2023-04-12T09:14:48.842011Z","shell.execute_reply.started":"2023-04-12T09:14:48.832227Z","shell.execute_reply":"2023-04-12T09:14:48.840928Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# Traning","metadata":{}},{"cell_type":"code","source":"def traning():\n    train_loader, val_loader = train_dataloader()\n    \n    model = ATDModel(vocab_size=len(word_to_index), tagset_size=1, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM)\n    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n    trainer = pl.Trainer(max_epochs=NUM_EPOCHS, callbacks=[early_stopping], accelerator='gpu', devices=1)\n    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n    \n    pickle.dump(model, open(\"lstm_model.sav\", 'wb'))\n    pickle.dump(word_to_index, open(\"word_to_index.sav\", 'wb'))\n    \n    print(\"Model Saved\")\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:14:48.843264Z","iopub.execute_input":"2023-04-12T09:14:48.843749Z","iopub.status.idle":"2023-04-12T09:14:48.853852Z","shell.execute_reply.started":"2023-04-12T09:14:48.843711Z","shell.execute_reply":"2023-04-12T09:14:48.852857Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"def testing():\n    global word_to_index\n    word_to_index = pickle.load(open(\"word_to_index.sav\", 'rb'))\n    model  = pickle.load(open(\"lstm_model.sav\", 'rb'))\n    \n    test_loader = test_dataloader()\n    \n    model.eval()\n\n    y_true = []\n    y_pred = []\n\n    with torch.no_grad():\n        for x,y in test_loader:\n            \n            # Forward pass\n            y_hat = model(x)\n            y_hat = y_hat > 0.5\n            # Compute the predicted tags\n            y_pred += y_hat.tolist()\n\n            #Compute the true tags\n            y_true += y.tolist()\n    \n    y_pred_df = pd.Series(y_pred)\n    result_csv = y_pred_df.to_csv(\"resultLSTM.csv\", index=False)\n    print(\"Result saved in resultLSTM.csv\")\n    print(classification_report(y_true, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:14:48.855415Z","iopub.execute_input":"2023-04-12T09:14:48.855830Z","iopub.status.idle":"2023-04-12T09:14:48.865590Z","shell.execute_reply.started":"2023-04-12T09:14:48.855793Z","shell.execute_reply":"2023-04-12T09:14:48.864429Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"if not TEST:\n    traning()\nelse:\n    testing()\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-12T09:14:48.867070Z","iopub.execute_input":"2023-04-12T09:14:48.867329Z","iopub.status.idle":"2023-04-12T09:14:55.490224Z","shell.execute_reply.started":"2023-04-12T09:14:48.867304Z","shell.execute_reply":"2023-04-12T09:14:55.489086Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","output_type":"stream"},{"name":"stdout","text":"Result saved in resultLSTM.csv\n              precision    recall  f1-score   support\n\n           0       0.79      0.76      0.77      3496\n           1       0.75      0.78      0.76      3232\n\n    accuracy                           0.77      6728\n   macro avg       0.77      0.77      0.77      6728\nweighted avg       0.77      0.77      0.77      6728\n\n","output_type":"stream"}]}]}